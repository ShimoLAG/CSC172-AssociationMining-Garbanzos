{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FoXAzrfspcFb"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import itertools\n",
        "\n",
        "plt.style.use('ggplot')\n",
        "\n",
        "# Ensure the CSV is in the same directory\n",
        "FILENAME = 'mobile_game_inapp_purchases.csv'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_and_preprocess(filepath):\n",
        "    print(\"--- Step 1 & 2: Loading and Preprocessing ---\")\n",
        "\n",
        "    try:\n",
        "        df = pd.read_csv(filepath)\n",
        "        print(f\"Original Dataset Shape: {df.shape}\")\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: '{filepath}' not found.\")\n",
        "        return None, None\n",
        "\n",
        "    # Handle Missing Values (Added .copy() to fix warning)\n",
        "    df_clean = df.dropna(subset=['Age', 'AverageSessionLength', 'Gender', 'Device', 'SpendingSegment']).copy()\n",
        "    print(f\"Shape after cleaning: {df_clean.shape}\")\n",
        "\n",
        "    # Feature Engineering\n",
        "    df_clean['Age_Group'] = pd.cut(df_clean['Age'], bins=[0, 18, 25, 35, 100],\n",
        "                                   labels=['Age:<18', 'Age:18-24', 'Age:25-34', 'Age:35+'])\n",
        "\n",
        "    df_clean['Session_Bin'] = pd.qcut(df_clean['AverageSessionLength'], q=3,\n",
        "                                      labels=['Session:Short', 'Session:Medium', 'Session:Long'])\n",
        "\n",
        "    # Create Transaction Baskets\n",
        "    basket_cols = ['Device', 'Gender', 'SpendingSegment', 'PaymentMethod', 'GameGenre', 'Age_Group', 'Session_Bin']\n",
        "\n",
        "    transactions = []\n",
        "    for i, row in df_clean.iterrows():\n",
        "        basket = []\n",
        "        for col in basket_cols:\n",
        "            if pd.notna(row[col]):\n",
        "                basket.append(f\"{col}={row[col]}\")\n",
        "        transactions.append(basket)\n",
        "\n",
        "    print(f\"Dataset loaded. Total Transactions: {len(transactions)}\")\n",
        "    return df_clean, transactions"
      ],
      "metadata": {
        "id": "1Xy50jHrpibN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def perform_eda(transactions):\n",
        "    print(\"\\n--- Step 3: Data Exploration (EDA) ---\")\n",
        "\n",
        "    all_items = [item for sublist in transactions for item in sublist]\n",
        "    item_counts = pd.Series(all_items).value_counts()\n",
        "\n",
        "    # Plot 1: Item Frequency\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    item_counts.head(15).sort_values().plot(kind='barh', color='teal')\n",
        "    plt.title('Top 15 Most Frequent Attributes')\n",
        "    plt.xlabel('Frequency')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Plot 2: Basket Size Distribution\n",
        "    transaction_sizes = [len(t) for t in transactions]\n",
        "    plt.figure(figsize=(8, 5))\n",
        "    plt.hist(transaction_sizes, bins=range(min(transaction_sizes), max(transaction_sizes) + 2),\n",
        "             align='left', rwidth=0.8, color='orange')\n",
        "    plt.title('Distribution of Basket Sizes')\n",
        "    plt.xlabel('Number of Items per User')\n",
        "    plt.ylabel('Count')\n",
        "    plt.show()\n",
        "\n",
        "    # Plot 3: Co-occurrence Heatmap\n",
        "    print(\"Generating Heatmap...\")\n",
        "    top_10 = item_counts.head(10).index.tolist()\n",
        "    matrix = pd.DataFrame(0, index=top_10, columns=top_10)\n",
        "\n",
        "    for basket in transactions:\n",
        "        basket_top = [item for item in basket if item in top_10]\n",
        "        for item1, item2 in itertools.combinations(basket_top, 2):\n",
        "            matrix.loc[item1, item2] += 1\n",
        "            matrix.loc[item2, item1] += 1\n",
        "\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.heatmap(matrix, annot=True, fmt='d', cmap='Blues')\n",
        "    plt.title('Co-occurrence Heatmap (Top 10 Items)')\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "RwJleqS9pk7M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_association_rules(transactions, min_support=0.03, min_confidence=0.20):\n",
        "    print(\"\\n--- Step 4: Association Rule Mining ---\")\n",
        "\n",
        "    N = len(transactions)\n",
        "    item_counts = {}\n",
        "    pair_counts = {}\n",
        "\n",
        "    # Frequent 1-Itemsets\n",
        "    for basket in transactions:\n",
        "        for item in basket:\n",
        "            item_counts[item] = item_counts.get(item, 0) + 1\n",
        "    frequent_items = {k: v for k, v in item_counts.items() if v/N >= min_support}\n",
        "\n",
        "    # Frequent 2-Itemsets\n",
        "    for basket in transactions:\n",
        "        basket_freq = [item for item in basket if item in frequent_items]\n",
        "        for pair in itertools.combinations(sorted(basket_freq), 2):\n",
        "            pair_counts[pair] = pair_counts.get(pair, 0) + 1\n",
        "    frequent_pairs = {k: v for k, v in pair_counts.items() if v/N >= min_support}\n",
        "\n",
        "    # Generate Rules & Metrics\n",
        "    rules = []\n",
        "    for pair, count_AB in frequent_pairs.items():\n",
        "        item_A, item_B = pair\n",
        "        support_AB = count_AB / N\n",
        "        support_A = frequent_items[item_A] / N\n",
        "        support_B = frequent_items[item_B] / N\n",
        "\n",
        "        def add_rule(antecedent, consequent, supp_ant, supp_cons):\n",
        "            confidence = support_AB / supp_ant\n",
        "            if confidence >= min_confidence:\n",
        "                lift = confidence / supp_cons\n",
        "                leverage = support_AB - (supp_ant * supp_cons)\n",
        "                conviction = float('inf') if confidence == 1 else (1 - supp_cons) / (1 - confidence)\n",
        "\n",
        "                rules.append({\n",
        "                    'Antecedent': antecedent,\n",
        "                    'Consequent': consequent,\n",
        "                    'Support': round(support_AB, 4),\n",
        "                    'Confidence': round(confidence, 4),\n",
        "                    'Lift': round(lift, 4),\n",
        "                    'Leverage': round(leverage, 4),\n",
        "                    'Conviction': round(conviction, 4)\n",
        "                })\n",
        "\n",
        "        add_rule(item_A, item_B, support_A, support_B)\n",
        "        add_rule(item_B, item_A, support_B, support_A)\n",
        "\n",
        "    rules_df = pd.DataFrame(rules)\n",
        "\n",
        "    if not rules_df.empty:\n",
        "        rules_df = rules_df.sort_values(by='Lift', ascending=False)\n",
        "        rules_df.to_csv('association_rules.csv', index=False)\n",
        "        print(f\"Success: {len(rules_df)} rules found. Saved to 'association_rules.csv'.\")\n",
        "        return rules_df\n",
        "    else:\n",
        "        print(\"No rules found. Try lowering thresholds.\")\n",
        "        return pd.DataFrame()"
      ],
      "metadata": {
        "id": "FSxe2OTOplis"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_metrics(rules_df):\n",
        "    print(\"\\n--- Step 5: Evaluation Visualization ---\")\n",
        "    plt.figure(figsize=(10, 6))\n",
        "\n",
        "    sns.scatterplot(\n",
        "        data=rules_df,\n",
        "        x=\"Support\",\n",
        "        y=\"Confidence\",\n",
        "        hue=\"Lift\",\n",
        "        size=\"Lift\",\n",
        "        sizes=(20, 200),\n",
        "        palette=\"viridis\"\n",
        "    )\n",
        "    plt.title('Association Rules: Support vs Confidence (Color = Lift)')\n",
        "    plt.xlabel('Support (Frequency)')\n",
        "    plt.ylabel('Confidence (Reliability)')\n",
        "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "ONqqi1CdpnJ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Load Data\n",
        "df, transactions = load_and_preprocess(FILENAME)\n",
        "\n",
        "if transactions:\n",
        "    # 2. Run EDA\n",
        "    perform_eda(transactions)\n",
        "\n",
        "    # 3. Mine Rules\n",
        "    rules_df = generate_association_rules(transactions, min_support=0.03, min_confidence=0.20)\n",
        "\n",
        "    # 4. Visualize Results\n",
        "    if not rules_df.empty:\n",
        "        plot_metrics(rules_df)\n",
        "        print(\"\\nTop 10 Rules by Lift:\")\n",
        "        print(rules_df.head(10).to_string(index=False))"
      ],
      "metadata": {
        "id": "LqB2BWA1poyD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}